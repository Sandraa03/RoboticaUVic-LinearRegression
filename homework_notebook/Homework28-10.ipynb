{
 "metadata": {
  "name": "",
  "signature": "sha256:622b0c127d4c7cdc55d7a8678f403df51d22766388bf0960d26e0c7c65df467e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q1) Download the Housing Data Set from the UCI repository (it is different from the one used in the notebook!).\n",
      "\n",
      "Q2) Load the data (using numpy.loadtxt) and separate the last column (target value, MEDV). Compute the average of the target value and the MSE obtained using it as a constant prediction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.linalg import lstsq\n",
      "\n",
      "data = np.loadtxt(\"data.txt\")\n",
      "y = data[:,-1]\n",
      "print 'Data points: ', len(y)\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#La mitajana dels elements de y es calcula fent la suma de tots els elements \n",
      "#i dividint per el nombre d'elements.\n",
      "m =  np.sum(y)/len(y)\n",
      "print 'Average of the target value: ', m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#np.dot serveix per calcular el producte de dos vectors\n",
      "#np.linalg.inv calcula la inversa d'una matriu\n",
      "#np.ones retorna una nova array de la forma dita plena de 1.\n",
      "#theta = (X'*X)^-1*X'*y\n",
      "#MSE = (1/N)*sum((y-X*theta)^2)\n",
      "\n",
      "def MSE(y,X,theta):\n",
      "    MSE = float(sum((y-dot(X, theta))**2)/len(y))\n",
      "    return MSE\n",
      "\n",
      "dot = np.dot\n",
      "inv = np.linalg.inv\n",
      "\n",
      "X = np.ones((len(y),1))\n",
      "\n",
      "theta = dot(dot(inv(dot(X.T, X)), X.T), y)\n",
      "\n",
      "#MSE = sum((y-dot(X, theta))**2)/len(y)\n",
      "\n",
      "print 'MSE = ', MSE(y,X,theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q3) Split the data in two parts (50%-50%) for training and testing (first half for training, second half for testing). Train a linear regressor model for each variable individually (plus a bias term) and compute the MSE on the training and the testing set. Which variable is the most informative? which one makes the model generalize better? and worse? Compute the coefficient of determination (R^2) for the test set.\n",
      "\n",
      "    Hint: If you want to select the i-th column of an array, but want it to retain the two dimension, you can do it like that:\n",
      "\n",
      "    column = data_array[:,i:i+1] \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Divim les dades en dues parts, la primera part per training i la segona \n",
      "#per testing i en separem la \u00faltima columna de cadascuna.\n",
      "\n",
      "rows_data_training = len(y)/2\n",
      "rows_data_testing = len(y) - rows_data_training\n",
      "\n",
      "data_training = data[:rows_data_training, :-1]\n",
      "data_testing = data[rows_data_training:, :-1]\n",
      "\n",
      "target_data_training = data[:rows_data_training, -1]\n",
      "target_data_testing = data[rows_data_training:, -1]\n",
      "\n",
      "#Regressi\u00f3 lineal per a cada variable individualment.\n",
      "#Creem arrays per emmagatzemar els resultats de cada variable\n",
      "\n",
      "training_theta = []\n",
      "training_MSE = []\n",
      "testing_MSE = []\n",
      "\n",
      "for i in range(0,len(data[0])-1):\n",
      "    #Training - Afegim variable amb Bias (X)\n",
      "    training_X = np.hstack((X[:rows_data_training], data_training[:,i].reshape(len(target_data_training),1)))\n",
      "    #Testing - Afegim variable amb Bias (X)\n",
      "    testing_X = np.hstack((X[rows_data_training:], data_testing[:,i].reshape(len(target_data_testing),1)))\n",
      "    #Training - Gaurdem el valor de theta per cada element\n",
      "    training_theta.append(lstsq(training_X, target_data_training)[0])\n",
      "    #Training - Gaurdem el MSE per a cada element\n",
      "    training_MSE.append(MSE(target_data_training, training_X, training_theta[i]))\n",
      "    #Testing - Gaurdem el MSE per a cada \n",
      "    testing_MSE.append(MSE(target_data_testing, testing_X, training_theta[i]))\n",
      "\n",
      "print 'Training MSE: '\n",
      "print training_MSE\n",
      "print ''\n",
      "print 'Testing MSE: ' \n",
      "print testing_MSE\n",
      "print ''\n",
      "\n",
      "#Most informative variable\n",
      "index_min = training_MSE.index(min(training_MSE)) \n",
      "print \"Most informative variable is number \", index_min, \"(counting from 0), with MSE \",training_MSE[index_min] \n",
      "print ''\n",
      "\n",
      "#Variable that generalize better the model\n",
      "index_min = testing_MSE.index(min(testing_MSE)) \n",
      "print \"Variable that generalize better the model is number \", index_min, \"(counting from 0), with MSE \", testing_MSE[index_min]\n",
      "print ''\n",
      "\n",
      "#Variable that generalize worse the model\n",
      "index_max = testing_MSE.index(max(testing_MSE)) \n",
      "print \"Variable that generalize worse the model is number \", index_max, \"(counting from 0), with MSE \", testing_MSE[index_max]\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per el set de testing\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean = sum(target_data_testing/len(target_data_testing))\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var = sum(mean-target_data_testing)**2/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu = testing_MSE/var\n",
      "#R2 = 1-fvu\n",
      "R2 = 1-fvu\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set: \"\n",
      "print R2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q4) Now train a model with all the variables plus a bias term. What is the performance in the test set? Try removing the worst-performing variable you found in step 3, and run again the experiment. What happened?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}