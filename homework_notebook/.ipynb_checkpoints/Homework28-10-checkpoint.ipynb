{
 "metadata": {
  "name": "",
  "signature": "sha256:a75c44dd00af25b9aadb27542ab10e7d8c3a511b8ca981a7b23a5f456e57dde1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q1) Download the Housing Data Set from the UCI repository (it is different from the one used in the notebook!).\n",
      "\n",
      "Q2) Load the data (using numpy.loadtxt) and separate the last column (target value, MEDV). Compute the average of the target value and the MSE obtained using it as a constant prediction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.linalg import lstsq\n",
      "\n",
      "data = np.loadtxt(\"data.txt\")\n",
      "y = data[:,-1]\n",
      "print 'Data points: ', len(y)\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#La mitajana dels elements de y es calcula fent la suma de tots els elements \n",
      "#i dividint per el nombre d'elements.\n",
      "m =  np.sum(y)/len(y)\n",
      "print 'Average of the target value: ', m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#np.dot serveix per calcular el producte de dos vectors\n",
      "#np.linalg.inv calcula la inversa d'una matriu\n",
      "#np.ones retorna una nova array de la forma dita plena de 1.\n",
      "#theta = (X'*X)^-1*X'*y\n",
      "#MSE = (1/N)*sum((y-X*theta)^2)\n",
      "\n",
      "def MSE(y,X,theta):\n",
      "    MSE = float(sum((y-dot(X, theta))**2)/len(y))\n",
      "    return MSE\n",
      "\n",
      "dot = np.dot\n",
      "inv = np.linalg.inv\n",
      "\n",
      "X = np.ones((len(y),1))\n",
      "\n",
      "theta = dot(dot(inv(dot(X.T, X)), X.T), y)\n",
      "\n",
      "#MSE = sum((y-dot(X, theta))**2)/len(y)\n",
      "\n",
      "print 'MSE = ', MSE(y,X,theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q3) Split the data in two parts (50%-50%) for training and testing (first half for training, second half for testing). Train a linear regressor model for each variable individually (plus a bias term) and compute the MSE on the training and the testing set. Which variable is the most informative? which one makes the model generalize better? and worse? Compute the coefficient of determination (R^2) for the test set.\n",
      "\n",
      "    Hint: If you want to select the i-th column of an array, but want it to retain the two dimension, you can do it like that:\n",
      "\n",
      "    column = data_array[:,i:i+1] \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Divim les dades en dues parts, la primera part per training i la segona \n",
      "#per testing i en separem la \u00faltima columna de cadascuna.\n",
      "\n",
      "rows_data_training = len(y)/2\n",
      "rows_data_testing = len(y) - rows_data_training\n",
      "\n",
      "data_training = data[:rows_data_training, :-1]\n",
      "data_testing = data[rows_data_training:, :-1]\n",
      "\n",
      "target_data_training = data[:rows_data_training, -1]\n",
      "target_data_testing = data[rows_data_training:, -1]\n",
      "\n",
      "#Regressi\u00f3 lineal per a cada variable individualment.\n",
      "#Creem arrays per emmagatzemar els resultats de cada variable\n",
      "\n",
      "training_theta = []\n",
      "training_MSE = []\n",
      "testing_MSE = []\n",
      "\n",
      "for i in range(0,len(data[0])-1):\n",
      "    #Training - Afegim variable amb Bias (X)\n",
      "    training_X = np.hstack((X[:rows_data_training], data_training[:,i].reshape(len(target_data_training),1)))\n",
      "    #Testing - Afegim variable amb Bias (X)\n",
      "    testing_X = np.hstack((X[rows_data_training:], data_testing[:,i].reshape(len(target_data_testing),1)))\n",
      "    #Training - Gaurdem el valor de theta per cada element\n",
      "    training_theta.append(lstsq(training_X, target_data_training)[0])\n",
      "    #Training - Gaurdem el MSE per a cada element\n",
      "    training_MSE.append(MSE(target_data_training, training_X, training_theta[i]))\n",
      "    #Testing - Gaurdem el MSE per a cada \n",
      "    testing_MSE.append(MSE(target_data_testing, testing_X, training_theta[i]))\n",
      "\n",
      "print 'Training MSE: '\n",
      "print training_MSE\n",
      "print ''\n",
      "print 'Testing MSE: ' \n",
      "print testing_MSE\n",
      "print ''\n",
      "\n",
      "#Most informative variable\n",
      "index_min = training_MSE.index(min(training_MSE)) \n",
      "print \"Most informative variable is number \", index_min, \"(counting from 0), with MSE \",training_MSE[index_min] \n",
      "print ''\n",
      "\n",
      "#Variable that generalize better the model\n",
      "index_min = testing_MSE.index(min(testing_MSE)) \n",
      "print \"Variable that generalize better the model is number \", index_min, \"(counting from 0), with MSE \", testing_MSE[index_min]\n",
      "print ''\n",
      "\n",
      "#Variable that generalize worse the model\n",
      "index_max = testing_MSE.index(max(testing_MSE)) \n",
      "print \"Variable that generalize worse the model is number \", index_max, \"(counting from 0), with MSE \", testing_MSE[index_max]\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per el set de testing\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean = sum(target_data_testing/len(target_data_testing))\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var = sum(mean-target_data_testing)**2/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu = testing_MSE/var\n",
      "#R2 = 1-fvu\n",
      "R2 = 1-fvu\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set: \"\n",
      "print R2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q4) Now train a model with all the variables plus a bias term. What is the performance in the test set? Try removing the worst-performing variable you found in step 3, and run again the experiment. What happened?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Creem array amb totes les variables amb el terme Bias (X)\n",
      "all_var = np.hstack((X[:rows_data_training], data_training[:,:].reshape(len(target_data_training),len(data[0])-1)))\n",
      "#Creem el model\n",
      "theta = (lstsq(all_var, target_data_training)[0])\n",
      "#Calculem MSE per al set de training amb la theta calculada amb totes les variables \n",
      "training_MSE_all_var = MSE(target_data_training, all_var, theta)\n",
      "#Calculem MSE per al set de testing amb la theta calculada amb totes les variables \n",
      "testing_MSE_all_var = MSE(target_data_testing, all_var, theta)\n",
      "\n",
      "print 'Training MSE for model with all variables: '\n",
      "print training_MSE_all_var\n",
      "print ''\n",
      "print 'Testing MSE for model with all variables: ' \n",
      "print testing_MSE_all_var\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per el set de testing amb el nou model\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean_all_var = sum(target_data_testing/len(target_data_testing))\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var_all_var = sum(mean-target_data_testing)**2/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu_all_var = testing_MSE_all_var/var_all_var\n",
      "#R2 = 1-fvu\n",
      "R2_all_var = 1-fvu_all_var\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set for the model with all variables: \"\n",
      "print R2_all_var\n",
      "print ''\n",
      "\n",
      "#Treiem la pitjor variable de les estimacions de Q3\n",
      "index_worst_var = testing_MSE.index(max(testing_MSE))\n",
      "\n",
      "if index_worst_var == 0:\n",
      "    data_training_Q4 = data_training[:, 1:]\n",
      "    data_testing_Q4 = data_testing[:, 1:]\n",
      "elif 0 < index_worst_var and index_worse_var < len(data[0])-1:\n",
      "    data_training_Q4 = data_training[:, range(0,index_worst_var)+range(index_worst_var+1,len(data[0])-1)]\n",
      "    data_testing_Q4 = data_testing[:, range(0,index_worst_var)+range(index_worst_var+1,len(data[0])-1)]\n",
      "elif index_worst_var == len(data[0])-1:\n",
      "    data_training_Q4 = data_training[:, :-1]\n",
      "    data_testing_Q4 = data_testing[:, :-1]\n",
      "    \n",
      "    \n",
      "#Tornem a calcular tots els valors\n",
      "\n",
      "#Creem array amb totes les variables, excepte la que em tret amb el terme Bias (X)\n",
      "all_var_Q4 = np.hstack((X[:rows_data_training], data_training_Q4[:,:].reshape(len(target_data_training),len(data[0])-2)))\n",
      "#Creem el model\n",
      "theta = (lstsq(all_var_Q4, target_data_training)[0])\n",
      "#Calculem MSE per al set de training amb la theta calculada amb totes les variables \n",
      "training_MSE_all_var_Q4 = MSE(target_data_training, all_var_Q4, theta)\n",
      "#Calculem MSE per al set de testing amb la theta calculada amb totes les variables \n",
      "testing_MSE_all_var_Q4 = MSE(target_data_testing, all_var_Q4, theta)\n",
      "\n",
      "print 'Training MSE for model with all variables except the worst: '\n",
      "print training_MSE_all_var_Q4\n",
      "print ''\n",
      "print 'Testing MSE for model with all variables except the worst: ' \n",
      "print testing_MSE_all_var_Q4\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per el set de testing amb el nou model\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean_all_var_Q4 = sum(target_data_testing/len(target_data_testing))\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var_all_var_Q4 = sum(mean-target_data_testing)**2/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu_all_var_Q4 = testing_MSE_all_var_Q4/var\n",
      "#R2 = 1-fvu\n",
      "R2_all_var_Q4 = 1-fvu_all_var_Q4\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set for the model with all variables except the worst: \"\n",
      "print R2_all_var_Q4\n",
      "print ''\n",
      "\n",
      "print 'This model is similar to the one with all variables'\n",
      "print 'R2 model with all variables: ', R2_all_var\n",
      "print 'R2 model with all variables except the worst: ', R2_all_var_Q4\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q5) We can give more capacity to a linear regression model by using basis functions (Bishop, sec. 3.1). In short, we can apply non-linear transformations to the input variables to extend the feature vector. Here we will try a polynomial function:\n",
      "\n",
      "     Polynomial basis expansion\n",
      "     \n",
      "Repeat step 3 but adding, one by one, all polynomials up to degree 4. What are the effects of adding more capacity to the model?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Regressi\u00f3 no-lineal\n",
      "#Creem arrays per emmagatzemar els resultats\n",
      "\n",
      "training_theta_no_lineal = []\n",
      "training_MSE_no_lineal = []\n",
      "testing_MSE_no_lineal = []\n",
      "\n",
      "for i in range(0,len(data[0])-1):\n",
      "    #Training - Afegim variable fins a x\u2074\n",
      "    x1_training = data_training[:,i].reshape(rows_data_training,1)\n",
      "    x2_training = data_training[:,i].reshape(rows_data_training,1) ** 2\n",
      "    x3_training = data_training[:,i].reshape(rows_data_training,1) ** 3\n",
      "    x4_training = data_training[:,i].reshape(rows_data_training,1) ** 4\n",
      "    \n",
      "    training_data_Q5 = np.hstack((X[:rows_data_training], x1_training, x2_training, x3_training, x4_training))\n",
      "    \n",
      "    training_theta_no_lineal.append(lstsq(training_data_Q5, target_data_training)[0])\n",
      "    \n",
      "    #Testing - Afegim variable fins a x\u2074\n",
      "    x1_testing = data_testing[:,i].reshape(rows_data_training,1)\n",
      "    x2_testing = data_testing[:,i].reshape(rows_data_training,1) ** 2\n",
      "    x3_testing = data_testing[:,i].reshape(rows_data_training,1) ** 3\n",
      "    x4_testing = data_testing[:,i].reshape(rows_data_training,1) ** 4\n",
      "    \n",
      "    testing_data_Q5 = np.hstack((X[:rows_data_training], x1_testing, x2_testing, x3_testing, x4_testing))\n",
      "    \n",
      "    training_MSE_no_lineal.append(MSE(target_data_training, training_data_Q5, training_theta_no_lineal[i]))\n",
      "    testing_MSE_no_lineal.append(MSE(target_data_testing, testing_data_Q5, training_theta_no_lineal[i]))\n",
      "       \n",
      "\n",
      "\n",
      "print 'Training MSE no lineal: '\n",
      "print training_MSE_no_lineal\n",
      "print ''\n",
      "print 'Testing MSE no lineal: ' \n",
      "print testing_MSE_no_lineal\n",
      "print ''\n",
      "\n",
      "#Most informative variable\n",
      "index_min = training_MSE_no_lineal.index(min(training_MSE_no_lineal))\n",
      "print \"Most informative variable is number \", index_min, \"(counting from 0), with MSE \",training_MSE_no_lineal[index_min] \n",
      "print ''\n",
      "\n",
      "#Variable that generalize better the model\n",
      "index_min = testing_MSE_no_lineal.index(min(testing_MSE_no_lineal)) \n",
      "print \"Variable that generalize better the model is number \", index_min, \"(counting from 0), with MSE \", testing_MSE_no_lineal[index_min]\n",
      "print ''\n",
      "\n",
      "#Variable that generalize worse the model\n",
      "index_max = testing_MSE_no_lineal.index(max(testing_MSE_no_lineal)) \n",
      "print \"Variable that generalize worse the model is number \", index_max, \"(counting from 0), with MSE \", testing_MSE_no_lineal[index_max]\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per al nou model no-lineal\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean_no_lineal = sum(target_data_testing/len(target_data_testing))\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var_no_lineal = sum(mean-target_data_testing)**2/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu_no_lineal = testing_MSE_no_lineal/var\n",
      "#R2 = 1-fvu\n",
      "R2_no_lineal = 1-fvu_no_lineal\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set: \"\n",
      "print R2_no_lineal\n",
      "\n",
      "print '' \n",
      "print 'In this case, results are similar to the ones on exercise Q3.'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q6) The objective function f for Regularized Linear Regression is the following:\n",
      "\n",
      "     f=Regularized Linear Regression\n",
      "     \n",
      "And its derivative f' is:\n",
      "\n",
      "     f-prime\n",
      "     \n",
      "Implement two functions in Python, one that computes f and another that computes f'. As an optional exercise, work the derivation of the objective function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Intentat, pero no m'en surto\n",
      "import random\n",
      "\n",
      "def regularized_linear_regression (theta, features, labels, Lambda):\n",
      "    \n",
      "    # f = (1/N) * (X*theta - y)^2 + Lambda*theta\n",
      "    f = (((dot(features, sum(theta[1:]) ** 2) - labels) ** 2) / len(y)) + Lambda * sum(theta[1:] ** 2)\n",
      "    \n",
      "    return f\n",
      "\n",
      "def derivate_regularized_linear_regression(theta, features, labels, Lambda):\n",
      "    \n",
      "    # f' = (2/N) * ((X^T)*(X*theta-y)) + 2*Lambda*theta\n",
      "    f_prima = ((2 * (features.transpose() * (dot(features, theta) - labels))) / len(labels)) + 2 * Lambda * theta\n",
      "    \n",
      "    return f_prima\n",
      "\n",
      "def Gradient_Descent():\n",
      "    \n",
      "    \n",
      "    maxit = 100\n",
      "    \n",
      "    #1.   Initialize theta(0) at random\n",
      "    theta = random.randint(1, maxit)\n",
      "    \n",
      "    print theta\n",
      "    \n",
      "    #2.   t=0, maxit=100, step=1e-6, loss=zeros(maxit)\n",
      "    t = 0\n",
      "    \n",
      "    step = 1e-6\n",
      "    #loss = zeros(maxit)\n",
      "    \n",
      "    #3.   loss(0) = f(theta)\n",
      "    loss = regularized_linear_regression(theta)\n",
      "    \n",
      "    #4.   do\n",
      "    #5.      t=t+1\n",
      "    #6.      theta(t) = theta(t-1) - step * f'(theta(t-1))\n",
      "    #7.      loss(t) = f(theta)\n",
      "    #8.   While t<maxit\n",
      "    #9.   return theta\n",
      "    \n",
      "    while (t<maxit):\n",
      "        theta[t] = theta[t-1] - step * derivate_regularized_linear_regression(theta[t-1])\n",
      "        loss[t] = regularized_linear_regression(theta[t])\n",
      "        t=t+1\n",
      "    \n",
      "    return theta[maxit]\n",
      "\n",
      "Gradient_Descent()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}