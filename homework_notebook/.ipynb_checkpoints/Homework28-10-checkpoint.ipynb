{
 "metadata": {
  "name": "",
  "signature": "sha256:6bbc05fc5f5ef90b22e6c7aa0c2472d94324ce4107b30aea0988368027b5bc8d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q1) Download the Housing Data Set from the UCI repository (it is different from the one used in the notebook!).\n",
      "\n",
      "Q2) Load the data (using numpy.loadtxt) and separate the last column (target value, MEDV). Compute the average of the target value and the MSE obtained using it as a constant prediction."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy.linalg import lstsq\n",
      "\n",
      "data = np.loadtxt(\"data.txt\")\n",
      "y = data[:,-1]\n",
      "print 'Data points: ', len(y)\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Data points:  506\n",
        "[ 24.   21.6  34.7  33.4  36.2  28.7  22.9  27.1  16.5  18.9  15.   18.9\n",
        "  21.7  20.4  18.2  19.9  23.1  17.5  20.2  18.2  13.6  19.6  15.2  14.5\n",
        "  15.6  13.9  16.6  14.8  18.4  21.   12.7  14.5  13.2  13.1  13.5  18.9\n",
        "  20.   21.   24.7  30.8  34.9  26.6  25.3  24.7  21.2  19.3  20.   16.6\n",
        "  14.4  19.4  19.7  20.5  25.   23.4  18.9  35.4  24.7  31.6  23.3  19.6\n",
        "  18.7  16.   22.2  25.   33.   23.5  19.4  22.   17.4  20.9  24.2  21.7\n",
        "  22.8  23.4  24.1  21.4  20.   20.8  21.2  20.3  28.   23.9  24.8  22.9\n",
        "  23.9  26.6  22.5  22.2  23.6  28.7  22.6  22.   22.9  25.   20.6  28.4\n",
        "  21.4  38.7  43.8  33.2  27.5  26.5  18.6  19.3  20.1  19.5  19.5  20.4\n",
        "  19.8  19.4  21.7  22.8  18.8  18.7  18.5  18.3  21.2  19.2  20.4  19.3\n",
        "  22.   20.3  20.5  17.3  18.8  21.4  15.7  16.2  18.   14.3  19.2  19.6\n",
        "  23.   18.4  15.6  18.1  17.4  17.1  13.3  17.8  14.   14.4  13.4  15.6\n",
        "  11.8  13.8  15.6  14.6  17.8  15.4  21.5  19.6  15.3  19.4  17.   15.6\n",
        "  13.1  41.3  24.3  23.3  27.   50.   50.   50.   22.7  25.   50.   23.8\n",
        "  23.8  22.3  17.4  19.1  23.1  23.6  22.6  29.4  23.2  24.6  29.9  37.2\n",
        "  39.8  36.2  37.9  32.5  26.4  29.6  50.   32.   29.8  34.9  37.   30.5\n",
        "  36.4  31.1  29.1  50.   33.3  30.3  34.6  34.9  32.9  24.1  42.3  48.5\n",
        "  50.   22.6  24.4  22.5  24.4  20.   21.7  19.3  22.4  28.1  23.7  25.\n",
        "  23.3  28.7  21.5  23.   26.7  21.7  27.5  30.1  44.8  50.   37.6  31.6\n",
        "  46.7  31.5  24.3  31.7  41.7  48.3  29.   24.   25.1  31.5  23.7  23.3\n",
        "  22.   20.1  22.2  23.7  17.6  18.5  24.3  20.5  24.5  26.2  24.4  24.8\n",
        "  29.6  42.8  21.9  20.9  44.   50.   36.   30.1  33.8  43.1  48.8  31.\n",
        "  36.5  22.8  30.7  50.   43.5  20.7  21.1  25.2  24.4  35.2  32.4  32.\n",
        "  33.2  33.1  29.1  35.1  45.4  35.4  46.   50.   32.2  22.   20.1  23.2\n",
        "  22.3  24.8  28.5  37.3  27.9  23.9  21.7  28.6  27.1  20.3  22.5  29.\n",
        "  24.8  22.   26.4  33.1  36.1  28.4  33.4  28.2  22.8  20.3  16.1  22.1\n",
        "  19.4  21.6  23.8  16.2  17.8  19.8  23.1  21.   23.8  23.1  20.4  18.5\n",
        "  25.   24.6  23.   22.2  19.3  22.6  19.8  17.1  19.4  22.2  20.7  21.1\n",
        "  19.5  18.5  20.6  19.   18.7  32.7  16.5  23.9  31.2  17.5  17.2  23.1\n",
        "  24.5  26.6  22.9  24.1  18.6  30.1  18.2  20.6  17.8  21.7  22.7  22.6\n",
        "  25.   19.9  20.8  16.8  21.9  27.5  21.9  23.1  50.   50.   50.   50.\n",
        "  50.   13.8  13.8  15.   13.9  13.3  13.1  10.2  10.4  10.9  11.3  12.3\n",
        "   8.8   7.2  10.5   7.4  10.2  11.5  15.1  23.2   9.7  13.8  12.7  13.1\n",
        "  12.5   8.5   5.    6.3   5.6   7.2  12.1   8.3   8.5   5.   11.9  27.9\n",
        "  17.2  27.5  15.   17.2  17.9  16.3   7.    7.2   7.5  10.4   8.8   8.4\n",
        "  16.7  14.2  20.8  13.4  11.7   8.3  10.2  10.9  11.    9.5  14.5  14.1\n",
        "  16.1  14.3  11.7  13.4   9.6   8.7   8.4  12.8  10.5  17.1  18.4  15.4\n",
        "  10.8  11.8  14.9  12.6  14.1  13.   13.4  15.2  16.1  17.8  14.9  14.1\n",
        "  12.7  13.5  14.9  20.   16.4  17.7  19.5  20.2  21.4  19.9  19.   19.1\n",
        "  19.1  20.1  19.9  19.6  23.2  29.8  13.8  13.3  16.7  12.   14.6  21.4\n",
        "  23.   23.7  25.   21.8  20.6  21.2  19.1  20.6  15.2   7.    8.1  13.6\n",
        "  20.1  21.8  24.5  23.1  19.7  18.3  21.2  17.5  16.8  22.4  20.6  23.9\n",
        "  22.   11.9]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#La mitajana dels elements de y es calcula fent la suma de tots els elements \n",
      "#i dividint per el nombre d'elements.\n",
      "m =  np.sum(y)/len(y)\n",
      "print 'Average of the target value: ', m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Average of the target value:  22.5328063241\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#np.dot serveix per calcular el producte de dos vectors\n",
      "#np.linalg.inv calcula la inversa d'una matriu\n",
      "#np.ones retorna una nova array de la forma dita plena de 1.\n",
      "#theta = (X'*X)^-1*X'*y\n",
      "#MSE = (1/N)*sum((y-X*theta)^2)\n",
      "\n",
      "def MSE(y,X,theta):\n",
      "    MSE = float(sum((y-dot(X, theta))**2)/len(y))\n",
      "    return MSE\n",
      "\n",
      "dot = np.dot\n",
      "inv = np.linalg.inv\n",
      "\n",
      "X = np.ones((len(y),1))\n",
      "\n",
      "theta = dot(dot(inv(dot(X.T, X)), X.T), y)\n",
      "\n",
      "#MSE = sum((y-dot(X, theta))**2)/len(y)\n",
      "\n",
      "print 'MSE = ', MSE(y,X,theta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MSE =  84.4195561562\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q3) Split the data in two parts (50%-50%) for training and testing (first half for training, second half for testing). Train a linear regressor model for each variable individually (plus a bias term) and compute the MSE on the training and the testing set. Which variable is the most informative? which one makes the model generalize better? and worse? Compute the coefficient of determination (R^2) for the test set.\n",
      "\n",
      "    Hint: If you want to select the i-th column of an array, but want it to retain the two dimension, you can do it like that:\n",
      "\n",
      "    column = data_array[:,i:i+1] \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Divim les dades en dues parts, la primera part per training i la segona \n",
      "#per testing i en separem la \u00faltima columna de cadascuna.\n",
      "\n",
      "rows_data_training = len(y)/2\n",
      "rows_data_testing = len(y) - rows_data_training\n",
      "\n",
      "data_training = data[:rows_data_training, :-1]\n",
      "data_testing = data[rows_data_training:, :-1]\n",
      "\n",
      "target_data_training = data[:rows_data_training, -1]\n",
      "target_data_testing = data[rows_data_training:, -1]\n",
      "\n",
      "#Regressi\u00f3 lineal per a cada variable individualment.\n",
      "#Creem arrays per emmagatzemar els resultats de cada variable\n",
      "\n",
      "training_theta = []\n",
      "training_MSE = []\n",
      "testing_MSE = []\n",
      "\n",
      "for i in range(0,len(data[0])-1):\n",
      "    #Training - Afegim variable amb Bias (X)\n",
      "    training_X = np.hstack((X[:rows_data_training], data_training[:,i].reshape(len(target_data_training),1)))\n",
      "    #Testing - Afegim variable amb Bias (X)\n",
      "    testing_X = np.hstack((X[rows_data_training:], data_testing[:,i].reshape(len(target_data_testing),1)))\n",
      "    #Training - Gaurdem el valor de theta per cada element\n",
      "    training_theta.append(lstsq(training_X, target_data_training)[0])\n",
      "    #Training - Gaurdem el MSE per a cada element\n",
      "    training_MSE.append(MSE(target_data_training, training_X, training_theta[i]))\n",
      "    #Testing - Gaurdem el MSE per a cada \n",
      "    testing_MSE.append(MSE(target_data_testing, testing_X, training_theta[i]))\n",
      "\n",
      "print 'Training MSE: '\n",
      "print training_MSE\n",
      "print ''\n",
      "print 'Testing MSE: ' \n",
      "print testing_MSE\n",
      "print ''\n",
      "\n",
      "#Most informative variable\n",
      "index_min = training_MSE.index(min(training_MSE)) \n",
      "print \"Most informative variable is number \", index_min, \"(counting from 0), with MSE \",training_MSE[index_min] \n",
      "print ''\n",
      "\n",
      "#Variable that generalize better the model\n",
      "index_min = testing_MSE.index(min(testing_MSE)) \n",
      "print \"Variable that generalize better the model is number \", index_min, \"(counting from 0), with MSE \", testing_MSE[index_min]\n",
      "print ''\n",
      "\n",
      "#Variable that generalize worse the model\n",
      "index_max = testing_MSE.index(max(testing_MSE)) \n",
      "print \"Variable that generalize worse the model is number \", index_max, \"(counting from 0), with MSE \", testing_MSE[index_max]\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per el set de testing\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean = sum(target_data_testing)/len(target_data_testing)\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var = sum((mean-target_data_testing)**2)/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu = testing_MSE/var\n",
      "#R2 = 1-fvu\n",
      "R2 = 1-fvu\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set: \"\n",
      "print R2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training MSE: \n",
        "[65.77339540438145, 62.18486412938459, 60.20294629831376, 69.24037546862544, 61.54527905641781, 15.935147920571659, 61.42749591725361, 68.00062307983848, 69.14935937656351, 65.43588160303844, 59.75097258260564, 66.04075276831176, 34.38075577589724]\n",
        "\n",
        "Testing MSE: \n",
        "[873.5168736238177, 91.93379466484049, 74.74611196598872, 105.20352304376996, 80.1601262254643, 78.05563442530612, 88.9553607232852, 97.46324219086713, 144.9052308496002, 67.51748098172573, 72.50716333055144, 86.44950014387534, 43.343723595296474]\n",
        "\n",
        "Most informative variable is number  5 (counting from 0), with MSE  15.9351479206\n",
        "\n",
        "Variable that generalize better the model is number  12 (counting from 0), with MSE  43.3437235953\n",
        "\n",
        "Variable that generalize worse the model is number  0 (counting from 0), with MSE  873.516873624\n",
        "\n",
        "Coefficient of determination (R^2) for the test set: \n",
        "[-8.3637876   0.01450269  0.19874849 -0.12774404  0.1407122   0.16327159\n",
        "  0.04643044 -0.044771   -0.5533321   0.27623682  0.22274921  0.07329236\n",
        "  0.53537083]\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q4) Now train a model with all the variables plus a bias term. What is the performance in the test set? Try removing the worst-performing variable you found in step 3, and run again the experiment. What happened?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Creem array amb totes les variables amb el terme Bias (X)\n",
      "all_var_training = np.hstack((X[:rows_data_training], data_training[:,:].reshape(len(target_data_training),len(data[0])-1)))\n",
      "all_var_testing = np.hstack((X[:rows_data_training], data_testing[:,:].reshape(len(target_data_testing),len(data[0])-1)))\n",
      "#Creem el model\n",
      "theta = (lstsq(all_var_training, target_data_training)[0])\n",
      "#Calculem MSE per al set de training amb la theta calculada amb totes les variables \n",
      "training_MSE_all_var = MSE(target_data_training, all_var_training, theta)\n",
      "#Calculem MSE per al set de testing amb la theta calculada amb totes les variables \n",
      "testing_MSE_all_var = MSE(target_data_testing, all_var_testing, theta)\n",
      "\n",
      "print 'Training MSE for model with all variables: '\n",
      "print training_MSE_all_var\n",
      "print ''\n",
      "print 'Testing MSE for model with all variables: ' \n",
      "print testing_MSE_all_var\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per el set de testing amb el nou model\n",
      "\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean_all_var = sum(target_data_testing)/len(target_data_testing)\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var_all_var = sum((mean-target_data_testing)**2)/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu_all_var = testing_MSE_all_var/var_all_var\n",
      "#R2 = 1-fvu\n",
      "R2_all_var = 1-fvu_all_var\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set for the model with all variables: \"\n",
      "print R2_all_var\n",
      "print ''\n",
      "\n",
      "#Treiem la pitjor variable de les estimacions de Q3\n",
      "index_worst_var = testing_MSE.index(max(testing_MSE))\n",
      "\n",
      "if index_worst_var == 0:\n",
      "    data_training_Q4 = data_training[:, 1:]\n",
      "    data_testing_Q4 = data_testing[:, 1:]\n",
      "elif 0 < index_worst_var and index_worse_var < len(data[0])-1:\n",
      "    data_training_Q4 = data_training[:, range(0,index_worst_var)+range(index_worst_var+1,len(data[0])-1)]\n",
      "    data_testing_Q4 = data_testing[:, range(0,index_worst_var)+range(index_worst_var+1,len(data[0])-1)]\n",
      "elif index_worst_var == len(data[0])-1:\n",
      "    data_training_Q4 = data_training[:, :-1]\n",
      "    data_testing_Q4 = data_testing[:, :-1]\n",
      "    \n",
      "    \n",
      "#Tornem a calcular tots els valors\n",
      "\n",
      "#Creem array amb totes les variables, excepte la que em tret amb el terme Bias (X)\n",
      "all_var_Q4 = np.hstack((X[:rows_data_training], data_training_Q4[:,:].reshape(len(target_data_training),len(data[0])-2)))\n",
      "#Creem el model\n",
      "theta = (lstsq(all_var_Q4, target_data_training)[0])\n",
      "#Calculem MSE per al set de training amb la theta calculada amb totes les variables \n",
      "training_MSE_all_var_Q4 = MSE(target_data_training, all_var_Q4, theta)\n",
      "#Calculem MSE per al set de testing amb la theta calculada amb totes les variables \n",
      "testing_MSE_all_var_Q4 = MSE(target_data_testing, all_var_Q4, theta)\n",
      "\n",
      "print 'Training MSE for model with all variables except the worst: '\n",
      "print training_MSE_all_var_Q4\n",
      "print ''\n",
      "print 'Testing MSE for model with all variables except the worst: ' \n",
      "print testing_MSE_all_var_Q4\n",
      "print ''\n",
      "\n",
      "#Coeficient R\u00b2 per el set de testing amb el nou model\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean_all_var_Q4 = sum(target_data_testing)/len(target_data_testing)\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var_all_var_Q4 = sum((mean-target_data_testing)**2)/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu_all_var_Q4 = testing_MSE_all_var_Q4/var\n",
      "#R2 = 1-fvu\n",
      "R2_all_var_Q4 = 1-fvu_all_var_Q4\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set for the model with all variables except the worst: \"\n",
      "print R2_all_var_Q4\n",
      "print ''\n",
      "\n",
      "print 'This model is similar to the one with all variables'\n",
      "print 'R2 model with all variables: ', R2_all_var\n",
      "print 'R2 model with all variables except the worst: ', R2_all_var_Q4\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training MSE for model with all variables: \n",
        "9.98751732546\n",
        "\n",
        "Testing MSE for model with all variables: \n",
        "303.436862927\n",
        "\n",
        "Coefficient of determination (R^2) for the test set for the model with all variables: \n",
        "-2.25273434239\n",
        "\n",
        "Training MSE for model with all variables except the worst: \n",
        "10.2162833878\n",
        "\n",
        "Testing MSE for model with all variables except the worst: \n",
        "190.939032479\n",
        "\n",
        "Coefficient of determination (R^2) for the test set for the model with all variables except the worst: \n",
        "-1.04679794755\n",
        "\n",
        "This model is similar to the one with all variables\n",
        "R2 model with all variables:  -2.25273434239\n",
        "R2 model with all variables except the worst:  -1.04679794755\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q5) We can give more capacity to a linear regression model by using basis functions (Bishop, sec. 3.1). In short, we can apply non-linear transformations to the input variables to extend the feature vector. Here we will try a polynomial function:\n",
      "\n",
      "     Polynomial basis expansion\n",
      "     \n",
      "Repeat step 3 but adding, one by one, all polynomials up to degree 4. What are the effects of adding more capacity to the model?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Regressi\u00f3 no-lineal\n",
      "#Creem arrays per emmagatzemar els resultats\n",
      "\n",
      "#MSE per Polinomi X\n",
      "\n",
      "training_theta_no_lineal_X = []\n",
      "training_MSE_no_lineal_X = []\n",
      "testing_MSE_no_lineal_X = []\n",
      "\n",
      "training_theta_no_lineal_X2 = []\n",
      "training_MSE_no_lineal_X2 = []\n",
      "testing_MSE_no_lineal_X2 = []\n",
      "\n",
      "training_theta_no_lineal_X3 = []\n",
      "training_MSE_no_lineal_X3 = []\n",
      "testing_MSE_no_lineal_X3 = []\n",
      "\n",
      "training_theta_no_lineal_X4 = []\n",
      "training_MSE_no_lineal_X4 = []\n",
      "testing_MSE_no_lineal_X4 = []\n",
      "\n",
      "for i in range(0,len(data[0])-1):\n",
      "    #Training - Afegim variable fins a x\u2074\n",
      "    x1_training = data_training[:,i].reshape(rows_data_training,1)\n",
      "    x2_training = data_training[:,i].reshape(rows_data_training,1) ** 2\n",
      "    x3_training = data_training[:,i].reshape(rows_data_training,1) ** 3\n",
      "    x4_training = data_training[:,i].reshape(rows_data_training,1) ** 4\n",
      "    \n",
      "    training_data_Q5_X = np.hstack((X[:rows_data_training], x1_training))\n",
      "    training_data_Q5_X2 = np.hstack((X[:rows_data_training], x1_training, x2_training))\n",
      "    training_data_Q5_X3 = np.hstack((X[:rows_data_training], x1_training, x2_training, x3_training))\n",
      "    training_data_Q5_X4 = np.hstack((X[:rows_data_training], x1_training, x2_training, x3_training, x4_training))\n",
      "    \n",
      "    training_theta_no_lineal_X.append(lstsq(training_data_Q5_X, target_data_training)[0])\n",
      "    training_theta_no_lineal_X2.append(lstsq(training_data_Q5_X2, target_data_training)[0])\n",
      "    training_theta_no_lineal_X3.append(lstsq(training_data_Q5_X3, target_data_training)[0])\n",
      "    training_theta_no_lineal_X4.append(lstsq(training_data_Q5_X4, target_data_training)[0])\n",
      "    \n",
      "\n",
      "    #Testing - Afegim variable fins a x\u2074\n",
      "    x1_testing = data_testing[:,i].reshape(rows_data_training,1)\n",
      "    x2_testing = data_testing[:,i].reshape(rows_data_training,1) ** 2\n",
      "    x3_testing = data_testing[:,i].reshape(rows_data_training,1) ** 3\n",
      "    x4_testing = data_testing[:,i].reshape(rows_data_training,1) ** 4\n",
      "    \n",
      "    testing_data_Q5_X = np.hstack((X[:rows_data_training], x1_testing))\n",
      "    testing_data_Q5_X2 = np.hstack((X[:rows_data_training], x1_testing, x2_testing))\n",
      "    testing_data_Q5_X3 = np.hstack((X[:rows_data_training], x1_testing, x2_testing, x3_testing))\n",
      "    testing_data_Q5_X4 = np.hstack((X[:rows_data_training], x1_testing, x2_testing, x3_testing, x4_testing))\n",
      "    \n",
      "    training_MSE_no_lineal_X.append(MSE(target_data_training, training_data_Q5_X, training_theta_no_lineal_X[i]))\n",
      "    training_MSE_no_lineal_X2.append(MSE(target_data_training, training_data_Q5_X2, training_theta_no_lineal_X2[i]))\n",
      "    training_MSE_no_lineal_X3.append(MSE(target_data_training, training_data_Q5_X3, training_theta_no_lineal_X3[i]))\n",
      "    training_MSE_no_lineal_X4.append(MSE(target_data_training, training_data_Q5_X4, training_theta_no_lineal_X4[i]))\n",
      "    \n",
      "    testing_MSE_no_lineal_X.append(MSE(target_data_testing, testing_data_Q5_X, training_theta_no_lineal_X[i]))\n",
      "    testing_MSE_no_lineal_X2.append(MSE(target_data_testing, testing_data_Q5_X2, training_theta_no_lineal_X2[i]))\n",
      "    testing_MSE_no_lineal_X3.append(MSE(target_data_testing, testing_data_Q5_X3, training_theta_no_lineal_X3[i]))\n",
      "    testing_MSE_no_lineal_X4.append(MSE(target_data_testing, testing_data_Q5_X4, training_theta_no_lineal_X4[i]))\n",
      "\n",
      "\n",
      "print 'Training MSE no lineal amb polinomi X: '\n",
      "print training_MSE_no_lineal_X\n",
      "print ''\n",
      "print 'Training MSE no lineal amb polinomi X\u00b2: '\n",
      "print training_MSE_no_lineal_X2\n",
      "print ''\n",
      "print 'Training MSE no lineal amb polinomi X\u00b3: '\n",
      "print training_MSE_no_lineal_X3\n",
      "print ''\n",
      "print 'Training MSE no lineal amb polinomi X\u2074: '\n",
      "print training_MSE_no_lineal_X4\n",
      "print ''\n",
      "print 'Testing MSE no lineal amb polinomi X: ' \n",
      "print testing_MSE_no_lineal_X\n",
      "print ''\n",
      "print 'Testing MSE no lineal amb polinomi X\u00b2: ' \n",
      "print testing_MSE_no_lineal_X2\n",
      "print ''\n",
      "print 'Testing MSE no lineal amb polinomi X\u00b3: ' \n",
      "print testing_MSE_no_lineal_X3\n",
      "print ''\n",
      "print 'Testing MSE no lineal amb polinomi X\u2074: ' \n",
      "print testing_MSE_no_lineal_X4\n",
      "print ''\n",
      "\n",
      "\n",
      "#Most informative variable\n",
      "index_min = training_MSE_no_lineal_X.index(min(training_MSE_no_lineal_X))\n",
      "print \"Most informative variable for X polynomial is number \", index_min, \"(counting from 0), with MSE \",training_MSE_no_lineal_X[index_min] \n",
      "print ''\n",
      "index_min = training_MSE_no_lineal_X2.index(min(training_MSE_no_lineal_X2))\n",
      "print \"Most informative variable for X\u00b2 polynomial is number \", index_min, \"(counting from 0), with MSE \",training_MSE_no_lineal_X2[index_min] \n",
      "print ''\n",
      "index_min = training_MSE_no_lineal_X3.index(min(training_MSE_no_lineal_X3))\n",
      "print \"Most informative variable for X\u00b3 polynomial is number \", index_min, \"(counting from 0), with MSE \",training_MSE_no_lineal_X3[index_min] \n",
      "print ''\n",
      "index_min = training_MSE_no_lineal_X4.index(min(training_MSE_no_lineal_X4))\n",
      "print \"Most informative variable for X\u2074 polynomial is number \", index_min, \"(counting from 0), with MSE \",training_MSE_no_lineal_X4[index_min] \n",
      "print ''\n",
      "\n",
      "#Variable that generalize better the model\n",
      "index_min = testing_MSE_no_lineal_X.index(min(testing_MSE_no_lineal_X)) \n",
      "print \"Variable that generalize better the model for X polynomial is number \", index_min, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X[index_min]\n",
      "print ''\n",
      "index_min = testing_MSE_no_lineal_X2.index(min(testing_MSE_no_lineal_X2)) \n",
      "print \"Variable that generalize better the model for X\u00b2 polynomial is number \", index_min, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X2[index_min]\n",
      "print ''\n",
      "index_min = testing_MSE_no_lineal_X3.index(min(testing_MSE_no_lineal_X3)) \n",
      "print \"Variable that generalize better the model for X\u00b3 polynomial is number \", index_min, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X3[index_min]\n",
      "print ''\n",
      "index_min = testing_MSE_no_lineal_X4.index(min(testing_MSE_no_lineal_X4)) \n",
      "print \"Variable that generalize better the model for X\u2074 polynomial is number \", index_min, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X4[index_min]\n",
      "print ''\n",
      "\n",
      "#Variable that generalize worse the model\n",
      "index_max = testing_MSE_no_lineal_X.index(max(testing_MSE_no_lineal_X)) \n",
      "print \"Variable that generalize worse the model for X polynomial is number \", index_max, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X[index_max]\n",
      "print ''\n",
      "index_max = testing_MSE_no_lineal_X2.index(max(testing_MSE_no_lineal_X2)) \n",
      "print \"Variable that generalize worse the model for X\u00b2 polynomial is number \", index_max, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X2[index_max]\n",
      "print ''\n",
      "index_max = testing_MSE_no_lineal_X3.index(max(testing_MSE_no_lineal_X3)) \n",
      "print \"Variable that generalize worse the model for X\u00b3 polynomial is number \", index_max, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X3[index_max]\n",
      "print ''\n",
      "index_max = testing_MSE_no_lineal_X4.index(max(testing_MSE_no_lineal_X4)) \n",
      "print \"Variable that generalize worse the model for X\u2074 polynomial is number \", index_max, \"(counting from 0), with MSE \", testing_MSE_no_lineal_X4[index_max]\n",
      "print ''\n",
      "\n",
      "\n",
      "#Coeficient R\u00b2 per al nou model no-lineal\n",
      "\n",
      "#mean = (1/N)*sum(y)\n",
      "mean_no_lineal = sum(target_data_testing)/len(target_data_testing)\n",
      "#var = (1/N)*sum((mean-y)\u00b2)\n",
      "var_no_lineal = sum((mean-target_data_testing)**2)/len(target_data_testing)\n",
      "#fvu = MSE/var\n",
      "fvu_no_lineal_X = testing_MSE_no_lineal_X/var\n",
      "fvu_no_lineal_X2 = testing_MSE_no_lineal_X2/var\n",
      "fvu_no_lineal_X3 = testing_MSE_no_lineal_X3/var\n",
      "fvu_no_lineal_X4 = testing_MSE_no_lineal_X4/var\n",
      "#R2 = 1-fvu\n",
      "R2_no_lineal_X = 1-fvu_no_lineal_X\n",
      "R2_no_lineal_X2 = 1-fvu_no_lineal_X2\n",
      "R2_no_lineal_X3 = 1-fvu_no_lineal_X3\n",
      "R2_no_lineal_X4 = 1-fvu_no_lineal_X4\n",
      "\n",
      "print \"Coefficient of determination (R^2) for the test set, X polynomial: \"\n",
      "print R2_no_lineal_X\n",
      "print ''\n",
      "print \"Coefficient of determination (R^2) for the test set, X\u00b2 polynomial: \"\n",
      "print R2_no_lineal_X2\n",
      "print ''\n",
      "print \"Coefficient of determination (R^2) for the test set, X\u00b3 polynomial: \"\n",
      "print R2_no_lineal_X3\n",
      "print ''\n",
      "print \"Coefficient of determination (R^2) for the test set, X\u2074 polynomial: \"\n",
      "print R2_no_lineal_X4\n",
      "print ''\n",
      "\n",
      "print '' \n",
      "print 'In this case, we can see that the results are better when polynomial is bigger. '\n",
      "print 'But the bigger the polynomial is, the bigger the work.'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training MSE no lineal amb polinomi X: \n",
        "[65.77339540438145, 62.18486412938459, 60.20294629831376, 69.24037546862544, 61.54527905641781, 15.935147920571659, 61.42749591725361, 68.00062307983848, 69.14935937656351, 65.43588160303844, 59.75097258260564, 66.04075276831176, 34.38075577589724]\n",
        "\n",
        "Training MSE no lineal amb polinomi X\u00b2: \n",
        "[65.61798913615462, 61.05821959474071, 54.24132564216832, 69.24061459906022, 61.29088930823114, 14.447074501123947, 60.26530556909271, 67.3816469855042, 63.19000726929228, 65.42900257944267, 57.20251562405998, 65.91101015133076, 23.44846110072957]\n",
        "\n",
        "Training MSE no lineal amb polinomi X\u00b3: \n",
        "[64.37154182163161, 60.92524988671504, 50.04489104080014, 69.24037991526575, 61.26738197199992, 13.804104769992653, 60.220277993595104, 64.72757965273112, 62.812572447500244, 64.53435032175065, 56.589927570332094, 65.67206641604749, 20.644036045474845]\n",
        "\n",
        "Training MSE no lineal amb polinomi X\u2074: \n",
        "[62.29064433894484, 59.94340248592256, 49.82500557532362, 69.24096353680726, 61.14210647832178, 13.322556345363639, 59.254252166275684, 63.57334657976435, 62.78585719950442, 63.34276030035417, 55.60500804229471, 65.35835442790774, 19.788874019602186]\n",
        "\n",
        "Testing MSE no lineal amb polinomi X: \n",
        "[873.5168736238177, 91.93379466484049, 74.74611196598872, 105.20352304376996, 80.1601262254643, 78.05563442530612, 88.9553607232852, 97.46324219086713, 144.9052308496002, 67.51748098172573, 72.50716333055144, 86.44950014387534, 43.343723595296474]\n",
        "\n",
        "Testing MSE no lineal amb polinomi X\u00b2: \n",
        "[125944.79202437177, 98.8542413115564, 73.23120626230983, 105.14671326116124, 78.5684512371279, 67.52845669828413, 87.32147234497555, 92.2374734396545, 34320.48281118505, 66.0689675665021, 77.04945951028466, 87.04882168572817, 43.30927624241118]\n",
        "\n",
        "Testing MSE no lineal amb polinomi X\u00b3: \n",
        "[4863293828.916368, 101.37965454859423, 85.84161368804025, 105.21129762380949, 80.66903113029993, 67.8211361362014, 87.0031120957421, 116.54198462896206, 520027.4478589685, 7106.9471156814, 76.6976442637144, 103.09535781802256, 42.4440172979375]\n",
        "\n",
        "Testing MSE no lineal amb polinomi X\u2074: \n",
        "[50071567633972.055, 103.84142844969175, 87.51560775969246, 105.11459477672447, 102.945221660976, 80.0841904882817, 87.72147213216235, 116.31840985184557, 720635.3452026589, 228316.98478882055, 68.09664287461185, 126.01732517294087, 40.99735047917611]\n",
        "\n",
        "Most informative variable for X polynomial is number  5 (counting from 0), with MSE  15.9351479206\n",
        "\n",
        "Most informative variable for X\u00b2 polynomial is number  5 (counting from 0), with MSE  14.4470745011\n",
        "\n",
        "Most informative variable for X\u00b3 polynomial is number  5 (counting from 0), with MSE  13.80410477\n",
        "\n",
        "Most informative variable for X\u2074 polynomial is number  5 (counting from 0), with MSE  13.3225563454\n",
        "\n",
        "Variable that generalize better the model for X polynomial is number  12 (counting from 0), with MSE  43.3437235953\n",
        "\n",
        "Variable that generalize better the model for X\u00b2 polynomial is number  12 (counting from 0), with MSE  43.3092762424\n",
        "\n",
        "Variable that generalize better the model for X\u00b3 polynomial is number  12 (counting from 0), with MSE  42.4440172979\n",
        "\n",
        "Variable that generalize better the model for X\u2074 polynomial is number  12 (counting from 0), with MSE  40.9973504792\n",
        "\n",
        "Variable that generalize worse the model for X polynomial is number  0 (counting from 0), with MSE  873.516873624\n",
        "\n",
        "Variable that generalize worse the model for X\u00b2 polynomial is number  0 (counting from 0), with MSE  125944.792024\n",
        "\n",
        "Variable that generalize worse the model for X\u00b3 polynomial is number  0 (counting from 0), with MSE  4863293828.92\n",
        "\n",
        "Variable that generalize worse the model for X\u2074 polynomial is number  0 (counting from 0), with MSE  5.0071567634e+13\n",
        "\n",
        "Coefficient of determination (R^2) for the test set, X polynomial: \n",
        "[-8.3637876   0.01450269  0.19874849 -0.12774404  0.1407122   0.16327159\n",
        "  0.04643044 -0.044771   -0.5533321   0.27623682  0.22274921  0.07329236\n",
        "  0.53537083]\n",
        "\n",
        "Coefficient of determination (R^2) for the test set, X\u00b2 polynomial: \n",
        "[ -1.34908300e+03  -5.96820126e-02   2.14987733e-01  -1.27135055e-01\n",
        "   1.57774381e-01   2.76119163e-01   6.39451345e-02   1.12473659e-02\n",
        "  -3.66903267e+02   2.91764363e-01   1.74057428e-01   6.68678517e-02\n",
        "   5.35740092e-01]\n",
        "\n",
        "Coefficient of determination (R^2) for the test set, X\u00b3 polynomial: \n",
        "[ -5.21327646e+07  -8.67535368e-02   7.98086885e-02  -1.27827376e-01\n",
        "   1.35256918e-01   2.72981745e-01   6.73578422e-02  -2.49288277e-01\n",
        "  -5.57350773e+03  -7.51839242e+01   1.77828761e-01  -1.05145260e-01\n",
        "   5.45015357e-01]\n",
        "\n",
        "Coefficient of determination (R^2) for the test set, X\u2074 polynomial: \n",
        "[ -5.36749247e+11  -1.13142870e-01   6.18640724e-02  -1.26790756e-01\n",
        "  -1.03535855e-01   1.41526200e-01   5.96572802e-02  -2.46891636e-01\n",
        "  -7.72395244e+03  -2.44647619e+03   2.70028411e-01  -3.50860530e-01\n",
        "   5.60523107e-01]\n",
        "\n",
        "\n",
        "In this case, we can see that the results are better when polynomial is bigger. \n",
        "But the bigger the polynomial is, the bigger the work.\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q6) The objective function f for Regularized Linear Regression is the following:\n",
      "\n",
      "     f=Regularized Linear Regression\n",
      "     \n",
      "And its derivative f' is:\n",
      "\n",
      "     f-prime\n",
      "     \n",
      "Implement two functions in Python, one that computes f and another that computes f'. As an optional exercise, work the derivation of the objective function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Intentat, pero no m'en surto\n",
      "import random\n",
      "\n",
      "def regularized_linear_regression (theta, features, labels, Lambda):\n",
      "    \n",
      "    # f = (1/N) * norm(X*theta - y)^2 + Lambda*norm(theta)\n",
      "    \n",
      "    #(1/N) * norm(X*theta - y)^2\n",
      "    t1 = dot(features, theta) - labels\n",
      "    t2 = dot(t1, t1.T) / len(labels)\n",
      "\n",
      "    #Lambda*norm(theta)\n",
      "    t3 = Lambda*dot(theta.T, theta)    \n",
      "    \n",
      "    f = t2 + t3\n",
      "    \n",
      "    return f\n",
      "\n",
      "def derivate_regularized_linear_regression(theta, features, labels, Lambda):\n",
      "    \n",
      "    # f' = (2/N) * ((X^T)*(X*theta-y)) + 2*Lambda*theta\n",
      "    \n",
      "    #(2/N) * ((X^T)*(X*theta-y))\n",
      "    t1 = dot(features, theta) - labels\n",
      "    t2 = dot(features.T, t1) * 2 / len(labels)\n",
      "    \n",
      "    #2*Lambda*theta\n",
      "    t3 = 2 * Lambda * theta\n",
      "    \n",
      "    \n",
      "    f_prima = t2 + t3\n",
      "    \n",
      "    return f_prima\n",
      "\n",
      "print regularized_linear_regression(np.zeros((all_var_training.shape[1],)), all_var_training, target_data_training, 1)\n",
      "\n",
      "print derivate_regularized_linear_regression(np.zeros((all_var_training.shape[1],)), all_var_training, target_data_training, 1)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "660.108300395\n",
        "[ -4.86150198e+01  -2.03998614e+01  -6.76578261e+02  -4.22271731e+02\n",
        "  -3.90434783e+00  -2.47029506e+01  -3.17485880e+02  -3.03381368e+03\n",
        "  -2.06060021e+02  -2.22990514e+02  -1.53278427e+04  -8.57690435e+02\n",
        "  -1.84761215e+04  -4.77468150e+02]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Gradient_Descent(features, labels, Lambda):\n",
      "     \n",
      "    \n",
      "    #1.   Initialize theta(0) at random\n",
      "    theta = np.zeros((100, len(labels))\n",
      "    #theta = np.random.random(len(labels))\n",
      "    \n",
      "    print theta\n",
      "    \n",
      "    #2.   t=0, maxit=100, step=1e-6, loss=zeros(maxit)\n",
      "    t = 0\n",
      "    maxit = 100\n",
      "    step = 1e-6\n",
      "    \n",
      "    #3.   loss(0) = f(theta)\n",
      "    loss = regularized_linear_regression(theta, features, labels, Lambda)\n",
      "    \n",
      "    #4.   do\n",
      "    #5.      t=t+1\n",
      "    #6.      theta(t) = theta(t-1) - step * f'(theta(t-1))\n",
      "    #7.      loss(t) = f(theta)\n",
      "    #8.   While t<maxit\n",
      "    #9.   return theta\n",
      "    \n",
      "    while (t<maxit):\n",
      "        theta[t] = theta[t-1] - step * derivate_regularized_linear_regression(theta[t-1], features, labels, Lambda)\n",
      "        loss[t] = regularized_linear_regression(theta[t], features, labels, Lambda)\n",
      "        t=t+1\n",
      "    \n",
      "    return theta\n",
      "\n",
      "Gradient_Descent()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}